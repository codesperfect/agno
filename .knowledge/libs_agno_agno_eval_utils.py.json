{"is_source_file": true, "format": "Python", "description": "This file contains utility functions for evaluation processes, including logging evaluation runs, asynchronously logging evaluation runs, and storing evaluation results in files. It interacts with external APIs and schemas related to evaluation runs and results.", "external_files": ["agno.api.evals", "agno.api.schemas.evals", "agno.utils.log"], "external_methods": ["agno.api.evals.async_create_eval_run", "agno.api.evals.create_eval_run", "agno.api.schemas.evals.EvalRunCreate", "agno.api.schemas.evals.EvalType", "agno.utils.log.log_debug", "agno.utils.log.logger", "json.dumps", "pathlib.Path.write_text"], "published": [], "classes": [], "methods": [{"name": "log_eval_run", "description": "Logs an evaluation run by calling the create_eval_run API with provided parameters."}, {"name": "async_log_eval_run", "description": "Asynchronously logs an evaluation run by calling the async_create_eval_run API with provided parameters."}, {"name": "store_result_in_file", "description": "Stores an evaluation result object into a specified file path, creating directories if necessary."}], "calls": ["agno.api.evals.create_eval_run", "agno.api.evals.async_create_eval_run", "json.dumps", "Path.write_text", "log_debug"], "search-terms": ["eval", "utils", "log_eval_run", "async_log_eval_run", "store_result_in_file"], "state": 2, "file_id": 510, "knowledge_revision": 2744, "git_revision": "4fa8d66b18b8fc5214cee45c6e57e0f5406ee43f", "ctags": [], "filename": "libs/agno/agno/eval/utils.py", "hash": "234e29fe693769785733e8b06ee0a521", "format-version": 4, "code-base-name": "https://github.com/codesperfect/agno.git:main", "revision_history": [{"2744": "4fa8d66b18b8fc5214cee45c6e57e0f5406ee43f"}]}