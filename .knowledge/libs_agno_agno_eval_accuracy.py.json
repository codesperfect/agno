{"is_source_file": true, "format": "Python", "description": "This file defines classes and functions related to evaluating the accuracy of AI agents or teams, including data structures for responses and evaluation results, as well as methods for running and logging evaluations.", "external_files": ["agno.agent", "agno.api.schemas.evals", "agno.eval.utils", "agno.exceptions", "agno.models.base", "agno.team.team", "agno.utils.log"], "external_methods": ["async_log_eval_run", "log_eval_run", "store_result_in_file", "set_log_level_to_debug", "set_log_level_to_info", "logger.exception", "logger.debug", "logger.error"], "published": ["AccuracyAgentResponse", "AccuracyEvaluation", "AccuracyResult", "AccuracyEval"], "classes": [{"name": "AccuracyAgentResponse", "description": "Data model for the response from an accuracy evaluation agent, including score and reasoning."}, {"name": "AccuracyEvaluation", "description": "Represents a single evaluation instance, including input, output, expected output, score, and reason, with a method to print the evaluation."}, {"name": "AccuracyResult", "description": "Aggregates multiple AccuracyEvaluation instances, computes statistics, and provides methods to print summaries and detailed results."}, {"name": "AccuracyEval", "description": "Main class to perform accuracy evaluation of an agent or team, including methods to run evaluations synchronously and asynchronously, with options to log and save results."}], "methods": [{"name": "print_eval", "description": "Prints the details of a single accuracy evaluation."}, {"name": "compute_stats", "description": "Calculates statistical metrics (mean, min, max, std deviation) for the evaluation results."}, {"name": "print_summary", "description": "Prints a summary of all evaluation results."}, {"name": "print_results", "description": "Prints detailed results of all evaluations."}, {"name": "get_evaluator_agent", "description": "Returns the evaluator agent, creating a default one if not provided."}, {"name": "get_eval_expected_output", "description": "Returns the expected output, calling if it's a callable."}, {"name": "get_eval_input", "description": "Returns the evaluation input, calling if it's a callable."}, {"name": "evaluate_answer", "description": "Synchronously evaluates the answer using the evaluator agent."}, {"name": "aevaluate_answer", "description": "Asynchronously evaluates the answer using the evaluator agent."}, {"name": "run", "description": "Runs the evaluation synchronously over specified iterations, logging, printing, and saving results."}, {"name": "arun", "description": "Runs the evaluation asynchronously over specified iterations, logging, printing, and saving results."}, {"name": "run_with_output", "description": "Runs evaluation against a provided output instead of generating one."}, {"name": "arun_with_output", "description": "Asynchronously runs evaluation against a provided output instead of generating one."}], "calls": ["agno.eval.utils.async_log_eval_run", "agno.eval.utils.log_eval_run", "agno.eval.utils.store_result_in_file", "set_log_level_to_debug", "set_log_level_to_info", "logger.exception", "logger.debug", "logger.error", "rich.console.Console", "rich.live.Live", "rich.status.Status", "rich.table.Table", "rich.box.ROUNDED", "rich.markdown.Markdown"], "search-terms": ["AccuracyEvaluation", "AccuracyResult", "AccuracyEval", "evaluation", "accuracy", "agent evaluation", "team evaluation", "evaluation logging", "accuracy scoring", "evaluation statistics"], "state": 2, "file_id": 509, "knowledge_revision": 2775, "git_revision": "75cfd531989fcd8b44f83d5dcf19cfb394daa15f", "ctags": [], "filename": "libs/agno/agno/eval/accuracy.py", "hash": "2fe68ee81ccdb240c527126b41797ebc", "format-version": 4, "code-base-name": "https://github.com/codesperfect/agno.git:main", "revision_history": [{"2775": "75cfd531989fcd8b44f83d5dcf19cfb394daa15f"}]}